# nvent_semantic_search.py (ë²ˆì—­ ì—ëŸ¬ ìˆ˜ì • ë²„ì „)
import streamlit as st
from io import BytesIO
import os
import time
import numpy as np
from collections import defaultdict
import re
from typing import Dict, Any, List, Set, Union

# --- 1. í™˜ê²½ ì„¤ì • ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ---

# í…ìŠ¤íŠ¸ ì¶”ì¶œ ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    from PyPDF2 import PdfReader
except ImportError:
    PdfReader = None
    st.warning("âš ï¸ PyPDF2 ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•´ì•¼ PDF íŒŒì¼ì„ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n```pip install pypdf2```")

try:
    import docx
except ImportError:
    docx = None
    st.warning("âš ï¸ python-docx ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•´ì•¼ DOCX íŒŒì¼ì„ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n```pip install python-docx```")

# ì‹œë§¨í‹± ê²€ìƒ‰ ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    from sentence_transformers import SentenceTransformer
    from sklearn.metrics.pairwise import cosine_similarity
    
    # ëª¨ë¸ ë¡œë“œ (ìºì‹±ì„ í†µí•´ ì¬ì‹¤í–‰ ì‹œ ë¡œë”© ì‹œê°„ ë‹¨ì¶•)
    @st.cache_resource
    def load_model():
        # ë‹¤êµ­ì–´ ì§€ì› ë° ì¢‹ì€ ì„±ëŠ¥ì˜ ëª¨ë¸ ì„ íƒ
        return SentenceTransformer('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')

    MODEL = load_model()
    SEMANTIC_LIBRARIES_LOADED = True
except ImportError:
    SEMANTIC_LIBRARIES_LOADED = False
    MODEL = None
    st.error("ğŸš¨ ì‹œë§¨í‹± ê²€ìƒ‰ í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ í•„ìš”:\n```pip install sentence-transformers scikit-learn```")

# --- â­ ìˆ˜ì •: ë¬´ë£Œ ë²ˆì—­ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ë° ìƒíƒœ ì €ì¥ ---
TRANSLATOR_OBJECTS = {}
AVAILABLE_TRANSLATORS = {}

# 1ì°¨ ì‹œë„: deep-translator (ê°€ì¥ ì•ˆì •ì )
try:
    from deep_translator import GoogleTranslator as DeepGoogleTranslator
    
    def get_deep_translator_object():
        return DeepGoogleTranslator(source='en', target='ko')
        
    TRANSLATOR_OBJECTS['deep_translator'] = get_deep_translator_object
    AVAILABLE_TRANSLATORS['Google Translate (deep-translator)'] = 'deep_translator'
except ImportError:
    pass

# 2ì°¨ ì‹œë„: googletrans
if not AVAILABLE_TRANSLATORS:
    try:
        from googletrans import Translator
        
        def get_googletrans_object():
            return Translator()
            
        TRANSLATOR_OBJECTS['googletrans'] = get_googletrans_object
        AVAILABLE_TRANSLATORS['Google Translate (googletrans)'] = 'googletrans'
    except ImportError:
        pass

if not AVAILABLE_TRANSLATORS:
    st.warning("âš ï¸ ë¬´ë£Œ ë²ˆì—­ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ í•„ìš”:\n```pip install deep-translator``` ë˜ëŠ” ```pip install googletrans==4.0.0-rc1```")
    FREE_TRANSLATOR_LOADED = False
else:
    FREE_TRANSLATOR_LOADED = True
# --- â­ ìˆ˜ì • ë ---


# --- 2. ì„¤ì • ---
st.set_page_config("ë¬¸ì„œ ê²€ìƒ‰ê¸°", layout="wide")
st.title("ğŸ” ë¬¸ì„œ ê²€ìƒ‰")

# session state ì´ˆê¸°í™”
if "docs" not in st.session_state:
    st.session_state["docs"] = []
if "search_history" not in st.session_state:
    st.session_state["search_history"] = []
# â­ ì¶”ê°€: í˜„ì¬ ì„ íƒëœ ë²ˆì—­ ë¼ì´ë¸ŒëŸ¬ë¦¬ ìƒíƒœ ì €ì¥
if "selected_translator_key" not in st.session_state:
    # ê¸°ë³¸ê°’ ì„¤ì •
    st.session_state["selected_translator_key"] = list(AVAILABLE_TRANSLATORS.values())[0] if AVAILABLE_TRANSLATORS else "ì—†ìŒ"
    
CHUNK_LENGTH = 300 # ì²­í¬ ê¸¸ì´ (í† í° ëŒ€ì‹  ê¸€ì ìˆ˜)
OVERLAP_LENGTH = 50 # ì²­í¬ ì˜¤ë²„ë©
SIMILARITY_THRESHOLD = 0.55 # ì„ê³„ê°’ ì„¤ì •
# ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ê²½ìš°, ì‹œë§¨í‹± ìœ ì‚¬ë„(ìµœëŒ€ 1.0)ë¥¼ ì´ˆê³¼í•˜ëŠ” ì ìˆ˜ë¥¼ ë¶€ì—¬í•˜ì—¬ í•­ìƒ ìƒìœ„ ì •ë ¬ë˜ë„ë¡ í•¨
LEXICAL_OVERRIDE_SCORE = 1.01 

# --- 3. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ---

def clean_text(text: str) -> str:
    """í…ìŠ¤íŠ¸ ì •ë¦¬: ê³µë°±, íŠ¹ìˆ˜ë¬¸ì ë“± ì •ë¦¬"""
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def get_pdf_pages_data(uploaded_file: BytesIO) -> List[Dict[str, Any]]:
    """PDF íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° í˜ì´ì§€ë³„ ë°ì´í„° ë°˜í™˜"""
    if not PdfReader:
        return []
    try:
        reader = PdfReader(uploaded_file)
        pages_data = []
        for i, page in enumerate(reader.pages):
            text = page.extract_text()
            cleaned_text = clean_text(text)
            if cleaned_text and len(cleaned_text) > 20: 
                pages_data.append({
                    "text": cleaned_text,
                    "page_num": i + 1
                })
        return pages_data
    except Exception as e:
        st.error(f"PDF ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        return []

def get_text_from_docx(uploaded_file: BytesIO) -> str:
    """DOCX íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    if not docx:
        return ""
    try:
        document = docx.Document(uploaded_file)
        text = "\n".join([p.text for p in document.paragraphs])
        return clean_text(text)
    except Exception as e:
        st.error(f"DOCX ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        return ""

def get_document_chunks(text: str, chunk_length: int=CHUNK_LENGTH, overlap: int=OVERLAP_LENGTH) -> List[str]:
    """í…ìŠ¤íŠ¸ë¥¼ ê²¹ì¹˜ëŠ” ë¶€ë¶„ê³¼ í•¨ê»˜ ì²­í¬ë¡œ ë¶„í• """
    chunks = []
    start = 0
    while start < len(text):
        end = start + chunk_length
        chunk = text[start:end].strip()
        if chunk:
            chunks.append(chunk)
        start += (chunk_length - overlap)
        if start < 0: 
            start = 0 
    return chunks

def process_file(uploaded_file: BytesIO):
    """ì—…ë¡œë“œëœ íŒŒì¼ ì²˜ë¦¬ ë° ì²­í¬ ìƒì„± (ìœ„ì¹˜ ì •ë³´ í¬í•¨)"""
    if uploaded_file.name in [doc['name'] for doc in st.session_state["docs"]]:
        st.warning(f"'{uploaded_file.name}' íŒŒì¼ì€ ì´ë¯¸ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
        return

    file_extension = os.path.splitext(uploaded_file.name)[1].lower()
    
    chunks_with_location = []
    raw_text_size = 0

    if file_extension == '.pdf':
        pages_data = get_pdf_pages_data(uploaded_file) 
        if not pages_data:
            st.error("PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆê±°ë‚˜ ë‚´ìš©ì´ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤.")
            return

        for page_data in pages_data:
            page_text = page_data['text']
            page_chunks = get_document_chunks(page_text)
            raw_text_size += len(page_text)

            for chunk_text in page_chunks:
                chunks_with_location.append({
                    "text": chunk_text,
                    "location": f"í˜ì´ì§€ {page_data['page_num']}" 
                })

    elif file_extension in ['.docx']:
        raw_text = get_text_from_docx(uploaded_file)
        if not raw_text: return
        
        raw_text_size = len(raw_text)
        base_chunks = get_document_chunks(raw_text)
        for i, chunk_text in enumerate(base_chunks):
            chunks_with_location.append({
                "text": chunk_text,
                "location": f"ë¸”ë¡ {i + 1}"
            })
            
    elif file_extension in ['.txt', '.md']:
        try:
            # â­ ì¸ì½”ë”© ì˜¤ë¥˜ê°€ ë°œìƒí•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ, 'errors='ignore'' ì¶”ê°€ (í…ìŠ¤íŠ¸ íŒŒì¼ ë¡œë”© ì•ˆì „ì„± í™•ë³´)
            raw_text = uploaded_file.getvalue().decode('utf-8', errors='ignore')
        except Exception as e:
            st.error(f"í…ìŠ¤íŠ¸ íŒŒì¼ ë””ì½”ë”© ì˜¤ë¥˜: {e}. íŒŒì¼ ì¸ì½”ë”©ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
            return
            
        if not raw_text:
            st.error("í…ìŠ¤íŠ¸ íŒŒì¼ ë‚´ìš©ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
            return
            
        raw_text = clean_text(raw_text)
        raw_text_size = len(raw_text)
        base_chunks = get_document_chunks(raw_text)
        for i, chunk_text in enumerate(base_chunks):
            chunks_with_location.append({
                "text": chunk_text,
                "location": f"ë¸”ë¡ {i + 1}"
            })
    
    if not chunks_with_location:
        st.warning("íŒŒì¼ì—ì„œ ìœ íš¨í•œ í…ìŠ¤íŠ¸ ì²­í¬ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    if raw_text_size < 50:
        st.warning("íŒŒì¼ ë‚´ìš©ì´ ë„ˆë¬´ ì§§ì•„ ì¸ë±ì‹±ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return
        
    # 2. ì„ë² ë”© ìƒì„±
    with st.spinner(f"'{uploaded_file.name}' ì„ë² ë”© ìƒì„± ì¤‘..."):
        chunk_texts = [chunk["text"] for chunk in chunks_with_location]
        embeddings = MODEL.encode(chunk_texts)
    
    # 3. ë¬¸ì„œ ìƒíƒœ ì €ì¥
    doc_data = {
        "name": uploaded_file.name,
        "size": raw_text_size,
        "chunks": []
    }
    for i in range(len(chunks_with_location)):
        doc_data["chunks"].append({
            "text": chunks_with_location[i]["text"],
            "embedding": embeddings[i],
            "location": chunks_with_location[i]["location"] 
        })
    st.session_state["docs"].append(doc_data)
    st.success(f"âœ… '{uploaded_file.name}' ë¡œë“œ ë° ì¸ë±ì‹± ì™„ë£Œ ({len(chunks_with_location)} ì²­í¬)")

def get_related_queries(query: str) -> List[str]:
    """
    ì¿¼ë¦¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê´€ë ¨ ì˜ì–´ ë²ˆì—­ ë° í•œêµ­ì–´/ì˜ì–´ ë™ì˜ì–´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    (í•˜ë“œì½”ë”©ëœ ë™ì˜ì–´ëŠ” ì‹¤ì œ APIë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš° ì œê±°í•´ì•¼ í•©ë‹ˆë‹¤.)
    """
    query_lower = query.lower()
    queries_to_run = [query] 
    
    # --- í•µì‹¬ í‚¤ì›Œë“œ/ë™ì˜ì–´ í™•ì¥ (Hard-coded Mocking) ---
    if 'ì¬ì§ˆ' in query_lower or 'ì†Œì¬' in query_lower:
        queries_to_run.extend(["material specifications", "composition", "alloy", "durability", "ë‚´êµ¬ì„±", "í•©ê¸ˆ", "ê·œê²©"])
    
    if 'ì¼€ì´ë¸”' in query_lower or 'ë°°ì„ ' in query_lower:
        queries_to_run.extend(["cable management solutions", "wiring diagram", "cabling", "ì „ì„ ", "ì ‘ì†"])
        
    if 'ì¸í´ë¡œì €' in query_lower or 'í•¨ì²´' in query_lower:
        queries_to_run.extend(["enclosure product standards", "housing", "protection rating", "NEMA", "IP rating", "ë³´í˜¸ ë“±ê¸‰"])

    if 'ì „ë ¥' in query_lower or 'ë°°ì „' in query_lower:
        queries_to_run.extend(["power distribution systems", "circuit breaker", "ì°¨ë‹¨ê¸°", "ë³€ì••ê¸°", "transformer"])

    if 'safety' in query_lower or 'ì•ˆì „' in query_lower:
        queries_to_run.extend(["safety regulations", "compliance", "ìœ„í—˜", "ê·œì • ì¤€ìˆ˜"])
    
    return list(set(queries_to_run))

def highlight_text(text: str, queries_to_highlight: List[str]) -> str:
    """í…ìŠ¤íŠ¸ ë‚´ì—ì„œ ì£¼ì–´ì§„ ì¿¼ë¦¬ë“¤ì„ <mark> íƒœê·¸ë¡œ í•˜ì´ë¼ì´íŠ¸í•©ë‹ˆë‹¤."""
    highlighted_text = text
    queries_to_highlight.sort(key=len, reverse=True) 
    
    for q_text in queries_to_highlight:
        q_text_stripped = clean_text(q_text)
        if not q_text_stripped: continue

        # ì •ê·œì‹ íŒ¨í„´ì„ ì‚¬ìš©í•˜ì—¬ ëŒ€ì†Œë¬¸ìë¥¼ êµ¬ë¶„í•˜ì§€ ì•Šê³  ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ë‹¨ì–´ë¥¼ ì°¾ì•„ êµì²´
        pattern = re.compile(re.escape(q_text_stripped), re.IGNORECASE)
        
        def replace_func(match):
            # ì›ë³¸ í…ìŠ¤íŠ¸ì˜ ëŒ€ì†Œë¬¸ìë¥¼ ìœ ì§€í•˜ë©´ì„œ í•˜ì´ë¼ì´íŠ¸
            return f"<mark>{match.group(0)}</mark>"
            
        highlighted_text = pattern.sub(replace_func, highlighted_text)
    
    return highlighted_text

def translate_text_free(text_to_translate: str, translator_key: str) -> str:
    """ì„ íƒëœ ë¬´ë£Œ ë²ˆì—­ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ í•œêµ­ì–´ë¡œ ë²ˆì—­í•©ë‹ˆë‹¤."""
    
    if not FREE_TRANSLATOR_LOADED or translator_key == "ì—†ìŒ":
        return "ë²ˆì—­ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. `deep-translator` ë˜ëŠ” `googletrans`ë¥¼ ì„¤ì¹˜í•´ì£¼ì„¸ìš”."
    
    try:
        if translator_key == "deep_translator":
            # deep-translator ì‚¬ìš© (ì•ˆì •ì )
            get_translator = TRANSLATOR_OBJECTS['deep_translator']
            translator = get_translator()
            
            # í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸¸ë©´ ë¶„í• 
            if len(text_to_translate) > 4500:
                sentences = text_to_translate.split('. ')
                translated_parts = []
                current_chunk = ""
                
                for sent in sentences:
                    if len(current_chunk) + len(sent) < 4500:
                        current_chunk += sent + ". "
                    else:
                        if current_chunk:
                            translated_parts.append(translator.translate(current_chunk))
                            time.sleep(0.3)
                        current_chunk = sent + ". "
                
                if current_chunk:
                    translated_parts.append(translator.translate(current_chunk))
                
                return " ".join(translated_parts)
            else:
                return translator.translate(text_to_translate)
            
        elif translator_key == "googletrans":
            # googletrans ì‚¬ìš©
            get_translator = TRANSLATOR_OBJECTS['googletrans']
            translator = get_translator()
            
            translation = translator.translate(text_to_translate, dest='ko')
            time.sleep(0.5)
            
            if not translation or not translation.text:
                return f"âš ï¸ ë²ˆì—­ ì‹¤íŒ¨: googletransê°€ ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
                
            return translation.text

        else:
            return f"ì•Œ ìˆ˜ ì—†ëŠ” ë²ˆì—­ ë¼ì´ë¸ŒëŸ¬ë¦¬ íƒ€ì…: {translator_key}"
            
    except Exception as e:
        error_name = e.__class__.__name__
        if 'JSONDecodeError' in error_name or 'HTTPError' in error_name:
             return f"ğŸš¨ ë²ˆì—­ ì˜¤ë¥˜ ë°œìƒ: ì„œë²„ ì°¨ë‹¨ ë˜ëŠ” API ë³€ê²½ìœ¼ë¡œ ì¸í•´ ë²ˆì—­ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ê±°ë‚˜ ë‹¤ë¥¸ ë²ˆì—­ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•´ ë³´ì„¸ìš”. ({error_name})"
        else:
            return f"ğŸš¨ ë²ˆì—­ ì˜¤ë¥˜ ë°œìƒ: {error_name} - {e}"


# --- 4. Streamlit UI êµ¬ì„± ---

# ì‚¬ì´ë“œë°”: íŒŒì¼ ì—…ë¡œë“œ ë° ë¬¸ì„œ ëª©ë¡
with st.sidebar:
    st.header("ğŸ“„ ë¬¸ì„œ ê´€ë¦¬")
    
    # --- â­ ë²ˆì—­ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„ íƒ ë“œë¡­ë‹¤ìš´ ì¶”ê°€ ---
    st.subheader("ğŸŒ ë²ˆì—­ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì •")
    
    if FREE_TRANSLATOR_LOADED:
        # ë“œë¡­ë‹¤ìš´ì—ì„œ ë³´ì—¬ì¤„ ë ˆì´ë¸” ëª©ë¡
        translator_labels = list(AVAILABLE_TRANSLATORS.keys())
        
        # ê¸°ë³¸ê°’ì„ í˜„ì¬ ì„¸ì…˜ ìƒíƒœì— ì €ì¥ëœ keyì— í•´ë‹¹í•˜ëŠ” labelë¡œ ì„¤ì •
        default_index = 0
        current_key = st.session_state["selected_translator_key"]
        
        # 'key'ë¥¼ 'label'ë¡œ ë³€í™˜
        key_to_label = {v: k for k, v in AVAILABLE_TRANSLATORS.items()}
        current_label = key_to_label.get(current_key, translator_labels[0] if translator_labels else "ì—†ìŒ")
        
        # í˜„ì¬ ë ˆì´ë¸”ì˜ ì¸ë±ìŠ¤ ì°¾ê¸°
        try:
            default_index = translator_labels.index(current_label)
        except ValueError:
            default_index = 0

        selected_label = st.selectbox(
            "ì‚¬ìš©í•  ë²ˆì—­ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„ íƒ",
            options=translator_labels,
            index=default_index,
            key="translator_select_widget" 
        )
        
        # ì„ íƒëœ ë ˆì´ë¸”ì— í•´ë‹¹í•˜ëŠ” ë‚´ë¶€ í‚¤(deep_translator ë˜ëŠ” googletrans)ë¥¼ ì°¾ì•„ ìƒíƒœì— ì €ì¥
        st.session_state["selected_translator_key"] = AVAILABLE_TRANSLATORS[selected_label]
        
    else:
        st.session_state["selected_translator_key"] = "ì—†ìŒ"
        st.warning("ì‚¬ìš© ê°€ëŠ¥í•œ ë²ˆì—­ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
    # --- â­ ë²ˆì—­ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„ íƒ ë ---
    
    st.markdown("---")
    
    # íŒŒì¼ ì—…ë¡œë“œ
    uploaded_files = st.file_uploader(
        "PDF, DOCX, TXT íŒŒì¼ ì—…ë¡œë“œ", 
        type=['pdf', 'docx', 'txt', 'md'], 
        accept_multiple_files=True
    )
    
    if uploaded_files:
        for uploaded_file in uploaded_files:
            process_file(uploaded_file)
            
    # ë¡œë“œëœ ë¬¸ì„œ ëª©ë¡
    st.markdown("### ğŸ“š ë¡œë“œëœ ë¬¸ì„œ ëª©ë¡")
    if st.session_state["docs"]:
        for doc in st.session_state["docs"]:
            chunk_count = len(doc['chunks'])
            st.caption(f"**{doc['name']}** ({chunk_count} ì²­í¬)")
        
        if st.button("ëª¨ë“  ë¬¸ì„œ ì œê±°"):
            st.session_state["docs"] = []
            st.session_state["search_history"] = []
            # ë²ˆì—­ ê²°ê³¼ ìƒíƒœë„ ì œê±°
            keys_to_delete = [k for k in st.session_state.keys() if k.startswith("translation_")]
            for k in keys_to_delete:
                del st.session_state[k]
            st.rerun()
    else:
        st.info("ì•„ì§ ë¡œë“œëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")

# --- 5. ì‹œë§¨í‹± ê²€ìƒ‰ ë¡œì§ ---

st.markdown("---")

if not SEMANTIC_LIBRARIES_LOADED:
    st.warning("ì‹œë§¨í‹± ê²€ìƒ‰ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ë¡œë“œë˜ì§€ ì•Šì•„ ê²€ìƒ‰ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
elif not st.session_state["docs"]:
    st.info("ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ë©´ ê²€ìƒ‰ì„ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
else:
    query = st.text_input(
        "ğŸ” ê²€ìƒ‰ì–´ ì…ë ¥ (ë¬¸ì„œ ë‚´ìš©ê³¼ ê´€ë ¨ëœ ì§ˆë¬¸ì´ë‚˜ í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”)",
        key="query_input"
    )
    
    col_settings, col_n_results, col_execute = st.columns([1, 1, 3])
    
    with col_settings:
        current_threshold = st.slider(
            "ìœ ì‚¬ë„ ì„ê³„ê°’", 
            min_value=0.0, 
            max_value=1.0, 
            value=SIMILARITY_THRESHOLD, 
            step=0.05,
            key="similarity_threshold_slider"
        )
    
    with col_n_results:
        N_RESULTS = st.number_input(
            "í‘œì‹œí•  ìµœëŒ€ ë¬¸ì„œ ê·¸ë£¹ ìˆ˜",
            min_value=1,
            max_value=50,
            value=10,
            step=1,
            key="n_results_input"
        )

    with col_execute:
        st.write("") 
        search_button = st.button("ê²€ìƒ‰ ì‹¤í–‰", type="primary", key="search_execute_button")

    # â­ ê²€ìƒ‰ ë²„íŠ¼ í´ë¦­ ì‹œì—ë§Œ ê²€ìƒ‰ ë¡œì§ ì‹¤í–‰
    if search_button and st.session_state["query_input"]:
        query = st.session_state["query_input"]
        
        if query not in st.session_state["search_history"]:
            st.session_state["search_history"].append(query)
            
        with st.spinner(f"'{query}'ì— ëŒ€í•œ ë¬¸ì„œë¥¼ í™•ì¥ ê²€ìƒ‰ ì¤‘..."):
            
            # --- 0. ê²€ìƒ‰ ì¿¼ë¦¬ ëª©ë¡ ë° ì„ë² ë”© ì¤€ë¹„ ---
            queries_to_run = get_related_queries(query)
            query_embeddings = {}
            for q_text in queries_to_run:
                try:
                    query_embeddings[q_text] = MODEL.encode(q_text) 
                except Exception as e:
                    st.error(f"'{q_text}' ì„ë² ë”© ìƒì„± ì˜¤ë¥˜: {e}")
                    st.stop()
            
            display_queries = ", ".join(queries_to_run)
            all_chunks_results = []
            
            # --- 2. ëª¨ë“  ì²­í¬ì— ëŒ€í•´ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ìˆ˜í–‰ ---
            for doc in st.session_state["docs"]:
                for chunk in doc.get("chunks", []):
                    
                    max_sim = -1.0
                    best_q_text = ""
                    is_lexical_override = False
                    
                    # 2-1. ì‹œë§¨í‹± ìœ ì‚¬ë„ ê³„ì‚°
                    for q_text, q_embedding in query_embeddings.items():
                        sim = cosine_similarity(
                            q_embedding.reshape(1, -1),
                            chunk["embedding"].reshape(1, -1)
                        )[0][0]
                        
                        if sim > max_sim:
                            max_sim = sim
                            best_q_text = q_text
                    
                    # 2-2. í‚¤ì›Œë“œ ì¼ì¹˜ í™•ì¸ (Lexical Check)
                    chunk_lower = clean_text(chunk["text"].lower())
                    matched_queries_for_override = [
                        q for q in queries_to_run 
                        if clean_text(q.lower()) in chunk_lower
                    ]
                    is_exact_match = len(matched_queries_for_override) > 0
                    
                    # 2-3. ê²°ê³¼ í¬í•¨ ê²°ì • ë° ì ìˆ˜ ë¶€ì—¬
                    if is_exact_match:
                        max_sim = LEXICAL_OVERRIDE_SCORE 
                        best_q_text = ", ".join(matched_queries_for_override) 
                        is_lexical_override = True
                    elif max_sim >= current_threshold:
                        pass 
                    else:
                        continue
                    
                    all_chunks_results.append({
                        "doc_name": doc["name"],
                        "text": chunk["text"],
                        "similarity": max_sim,
                        "location": chunk["location"],
                        "best_query_text": best_q_text, 
                        "is_lexical_override": is_lexical_override, 
                        "all_search_queries": queries_to_run 
                    })
                            
            # 3. ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬ (ê°œë³„ ì²­í¬ ê¸°ì¤€)
            all_chunks_results.sort(key=lambda x: x["similarity"], reverse=True)
            
            # 4. ê²°ê³¼ ê·¸ë£¹í™” (íŒŒì¼ëª… ê¸°ì¤€)
            grouped_results: Dict[str, List[Dict[str, Union[str, float, bool, List[str]]]]] = defaultdict(list)
            for r in all_chunks_results:
                group_key = r['doc_name']
                grouped_results[group_key].append(r)
                
            sorted_groups = []
            for doc_name, chunks in grouped_results.items():
                max_group_sim = max(c['similarity'] for c in chunks)
                group_is_lexical_override = any(c['is_lexical_override'] for c in chunks)
                
                sorted_groups.append({
                    "doc_name": doc_name,
                    "max_sim": max_group_sim,
                    "is_lexical_override": group_is_lexical_override,
                    "chunks": chunks
                })
            
            # ìµœì¢… ì •ë ¬: ì •í™•ë„ ì¼ì¹˜ ì—¬ë¶€ -> ìµœê³  ìœ ì‚¬ë„ ìˆœ
            sorted_groups.sort(key=lambda x: (x["is_lexical_override"], x["max_sim"]), reverse=True)
            
            # ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì„¸ì…˜ ìƒíƒœì— ì €ì¥
            st.session_state["last_search_results"] = sorted_groups
            st.session_state["search_performed"] = True
    
    # --- 6. ê²€ìƒ‰ ê²°ê³¼ ì¶œë ¥ ---
    if st.session_state.get("search_performed", False):
        sorted_groups = st.session_state["last_search_results"]

        st.subheader(f"ì´ {len(sorted_groups)}ê°œì˜ ë¬¸ì„œ ê·¸ë£¹ (ì„ê³„ê°’: {current_threshold}, ì •í™•ë„ ìš°ì„  ì •ë ¬)")
        st.markdown(f"**â„¹ï¸ í™•ì¥ ê²€ìƒ‰ ì •ë³´:** ì›ë³¸ ì¿¼ë¦¬: '{st.session_state['query_input']}'. ê²€ìƒ‰ì— ì‚¬ìš©ëœ í™•ì¥ ì¿¼ë¦¬/ë™ì˜ì–´: **{', '.join(get_related_queries(st.session_state['query_input']))}**")
        
        if not sorted_groups:
            st.info("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ì„ê³„ê°’ì„ ë‚®ì¶”ê±°ë‚˜ ë‹¤ë¥¸ ê²€ìƒ‰ì–´ë¥¼ ì‚¬ìš©í•´ ë³´ì„¸ìš”.")
        
        for idx, group in enumerate(sorted_groups[:int(N_RESULTS)]):
            group['chunks'].sort(key=lambda x: x["similarity"], reverse=True)

            match_tag = " [â­ï¸ ì •í™•íˆ ì¼ì¹˜]" if group['is_lexical_override'] else ""
            
            expander_title = (
                f"âœ¨ ìµœê³  ìœ ì‚¬ë„: {group['max_sim']:.3f}{match_tag} | íŒŒì¼ëª…: **{group['doc_name']}**"
            )

            with st.expander(expander_title, expanded=False):
                
                for chunk_idx, r in enumerate(group['chunks']):
                    
                    # â­ ë²ˆì—­ ê²°ê³¼ë¥¼ ì €ì¥í•  ê³ ìœ  í‚¤ ì •ì˜
                    translation_state_key = f"translation_{group['doc_name']}_{idx}_{chunk_idx}"
                    
                    # --- ì›ë¬¸ ìŠ¤ë‹ˆí« í•˜ì´ë¼ì´íŒ… ì²˜ë¦¬ ---
                    highlighted_text = highlight_text(r['text'], r['all_search_queries'])
                    
                    st.markdown("---")
                    
                    # ì²­í¬ ì œëª© (ìœ„ì¹˜ ì •ë³´ í¬í•¨) ë° ê¸°ì—¬ ì¿¼ë¦¬ í‘œì‹œ
                    chunk_header = (
                        f"**{r['location']}** | ì²­í¬ {chunk_idx + 1} (ìœ ì‚¬ë„: {r['similarity']:.3f})"
                    )
                    st.markdown(f"{chunk_header} | ê¸°ì—¬ ì¿¼ë¦¬: *{r['best_query_text']}*")
                    
                    # ì›ë¬¸ ìŠ¤ë‹ˆí« í‘œì‹œ
                    st.markdown("#### ğŸ“– ì›ë¬¸") 
                    st.markdown(highlighted_text, unsafe_allow_html=True) 
                    
                    # ë²ˆì—­ ì„¹ì…˜
                    st.markdown("#### ğŸŒ ì›ë¬¸ ë²ˆì—­")
                    
                    col_btn, col_placeholder = st.columns([1, 4])
                    
                    # 1. ë²ˆì—­ ë²„íŠ¼: ë²„íŠ¼ í´ë¦­ ì‹œ ë²ˆì—­ ìˆ˜í–‰ ë° ìƒíƒœ ì €ì¥
                    def handle_translation_wrapper(text_to_translate, key):
                        # st.session_stateì—ì„œ í•„ìš”í•œ ê°’ì„ ì½ì–´ì˜´
                        selected_key = st.session_state["selected_translator_key"]
                        # ë²ˆì—­ í•¨ìˆ˜ í˜¸ì¶œ
                        with st.spinner(f"ë¬´ë£Œ ë²ˆì—­ ë¼ì´ë¸ŒëŸ¬ë¦¬ ({selected_key})ë¥¼ ì‚¬ìš©í•˜ì—¬ ë²ˆì—­ ì¤‘..."):
                            translated_text = translate_text_free(text_to_translate, selected_key)
                            # ìƒíƒœ ì—…ë°ì´íŠ¸
                            st.session_state[key] = translated_text
                        
                    # ë²„íŠ¼ì„ í´ë¦­í•˜ë©´ ìœ„ í•¸ë“¤ëŸ¬ë¥¼ í˜¸ì¶œí•˜ë„ë¡ ì„¤ì •
                    if col_btn.button(
                        f"âœ¨ í•œêµ­ì–´ë¡œ ë²ˆì—­ ({st.session_state.get('selected_translator_key', 'ì—†ìŒ')})", 
                        key=f"translate_btn_{translation_state_key}",
                        on_click=handle_translation_wrapper,
                        args=(r['text'], translation_state_key)
                    ):
                        pass
                        
                    # 2. ë²ˆì—­ ê²°ê³¼ í‘œì‹œ: ì„¸ì…˜ ìƒíƒœì— ì €ì¥ëœ ê²°ê³¼ í‘œì‹œ
                    if translation_state_key in st.session_state:
                        col_placeholder.info(st.session_state[translation_state_key])
                    else:
                        col_placeholder.caption("ë²ˆì—­ì„ ë³´ë ¤ë©´ 'í•œêµ­ì–´ë¡œ ë²ˆì—­' ë²„íŠ¼ì„ ëˆ„ë¥´ì„¸ìš”.")
                
        st.markdown("---")

# --- ê²€ìƒ‰ íˆìŠ¤í† ë¦¬ (ì‚¬ì´ë“œë°” ìœ ì§€) ---
if st.session_state.get("search_history"):
    with st.sidebar:
        st.markdown("---")
        st.markdown("### ğŸ• ìµœê·¼ ê²€ìƒ‰")
        for h in reversed(st.session_state["search_history"][-5:]):
            st.caption(h)

